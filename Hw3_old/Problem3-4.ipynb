{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import h5py \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Forward(X,W):\n",
    "    S = np.matmul(W.T, X)\n",
    "    X = np.tanh(S)\n",
    "    return X, S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forwardPropagation(X0,W):\n",
    "    X_array = [] \n",
    "    S_array = []\n",
    "    X_array.append(X0)\n",
    "    layer_count = len(W)\n",
    "    for l in range (0, layer_count):\n",
    "        if(l == 0):\n",
    "            X,S = Forward(X0,W[l])\n",
    "            X = np.vstack([[1], X])\n",
    "            \n",
    "        elif(l ==layer_count-1):\n",
    "            X,S = Forward(X_array[l],W[l])\n",
    "            \n",
    "        else:\n",
    "            X,S = Forward(X_array[l],W[l])\n",
    "            X = np.vstack([[1], X])\n",
    "            \n",
    "        X_array.append(X)\n",
    "        S_array.append(S)\n",
    "    return X_array,S_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh_prime(X):\n",
    "    result = 1- X**2\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pointwise_theta(X):\n",
    "    result = 1 - np.multiply(X,X)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pointwise(X,Y):\n",
    "    result = np.multiply(X,Y)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backwardPropagation(X,S,W,Y):\n",
    "    delta_array = []\n",
    "    layer_count = len(W)\n",
    "    i = 0\n",
    "    for l in range (layer_count,0,-1):\n",
    "        if(l == layer_count):\n",
    "            delta = 2*(X[l]-Y) * tanh_prime(X[l])\n",
    "            \n",
    "        else:\n",
    "            if(len(delta_array[i])>1):\n",
    "                delta_temp = np.delete(delta_array[i],0)\n",
    "            else:\n",
    "                delta_temp = delta_array[i]\n",
    "            pointwise_X = pointwise_theta(X[l])\n",
    "            pointwise_delta_weight = pointwise(delta_temp,W[l])\n",
    "            delta = pointwise(pointwise_X, pointwise_delta_weight)\n",
    "            i+=1\n",
    "        delta_array.append(delta)\n",
    "    return delta_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def integration(W, X,Y):\n",
    "    G_array= []\n",
    "    Ein = 0\n",
    "    layer_count = len(W_array)\n",
    "    for l in range(0, len(W_array)): #initial G_array\n",
    "        temp = 0*W_array[l] \n",
    "        G_array.append(temp)\n",
    "    for i in range(0,len(X)):\n",
    "        X_array,S_array = forwardPropagation(X[i],W_array)\n",
    "        delta_array = backwardPropagation(X_array,S_array,W_array,Y[i])\n",
    "        \n",
    "       \n",
    "        Ein = Ein + 1/layer_count*(X_array[layer_count]-Y[i])**2\n",
    "        print(\"Ein\",Ein)\n",
    "        for l in range (0,layer_count):\n",
    "            if(len(delta_array[l])>1):\n",
    "#                 print(\"dealta array \",l,delta_array[l])\n",
    "                delta_temp = np.delete(delta_array[l],0, 0)\n",
    "                delta_temp = delta_temp.T\n",
    "#                 print(\"delta_temp \",l,delta_temp)\n",
    "            else:\n",
    "                delta_temp = delta_array[l]\n",
    "\n",
    "            dw = delta_temp*X_array[layer_count-l-1]\n",
    "\n",
    "            G_array[layer_count-l-1] = G_array[layer_count-l-1] + 1/layer_count*dw\n",
    "    return Ein, G_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(W_array,Data_set_X,Data_set_Y,learning_rate = 0.5,iteration = 10):\n",
    "    for iterate in range (0,iteration):\n",
    "        Ein , G = integration(W_array, Data_set_X,Data_set_Y)\n",
    "        for l in range(0, len(W_array)):\n",
    "            W_array[l] = W_array[l]-learning_rate*G[l]\n",
    "        print(\"iterate \",iterate,\"learning\\n\",W_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialWeigths(NEXT_LAYER_SIZE, INPUT_LAYER_SIZE):\n",
    "    W = np.random.randn(INPUT_LAYER_SIZE, NEXT_LAYER_SIZE) * \\\n",
    "                np.sqrt(2.0/INPUT_LAYER_SIZE)*0.01\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[1],[2],[3]]) \n",
    "X1 = np.array([[1],[2],[3]]) \n",
    "Y = np.array([[1]])\n",
    "Y1 = np.array([[2]])\n",
    "# W1 =np.array([[0.1,0.2],[0.3,0.4]])\n",
    "# W2 =np.array([[0.2],[1],[-3]])\n",
    "# W3 =np.array([[1],[2]])\n",
    "# W2 =np.array([[0.2,0.5],[1,4],[-3,5]])#2nuron in next layer and 3 nuron in this layer(include bias)\n",
    "# W3 =np.array([[1],[2],[3]])\n",
    "# W1_ = initialWeigths(2,3) #first index is the nuron in next layer(without bias) and second is the nuron in this layer(include bias)\n",
    "                            #first index is M and\n",
    "\n",
    "# W1 = initialWeigths(4,3)\n",
    "# W2 = initialWeigths(1,5)\n",
    "# print(W1)\n",
    "# print(W2)\n",
    "# Data_set_X = []\n",
    "# Data_set_X.append(X)\n",
    "# Data_set_X.append(X1)\n",
    "# Data_set_Y = []\n",
    "# Data_set_Y.append(Y)\n",
    "# Data_set_Y.append(Y1)\n",
    "# print(Data_set_X)\n",
    "# print(Data_set_Y)\n",
    "# W_array = []\n",
    "# W_array.append(W1)\n",
    "# W_array.append(W2)\n",
    "# W_array.append(W3)\n",
    "# learning_rate = 0.5\n",
    "# for iterate in range (0,1):\n",
    "#     Ein , G = integration(W_array, Data_set_X,Data_set_Y)\n",
    "#     for l in range(0, len(W_array)):\n",
    "#         W_array[l] = W_array[l]-learning_rate*G[l]\n",
    "#     print(\"iterate \",iterate,\"learning\\n\",W_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_gradient_descent(W_array,Data_set_X,Data_set_Y,learning_rate = 1,iteration = 11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import data from HW digits set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (20.0, 10.0) \n",
    "%matplotlib inline\n",
    "\n",
    "training_data1_df = pd.read_csv(\"ZipDigits_traning.csv\") # laod data from ZipDigits.training\n",
    "                                                         # to do this step I firstly converted the file to csv file\n",
    "training_data2_df = pd.read_csv(\"ZipDigits_test.csv\")\n",
    "\n",
    "training_data1_df.shape\n",
    "training_data1_df = training_data1_df.astype(float)\n",
    "\n",
    "training_data2_df.shape\n",
    "training_data2_df = training_data2_df.astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare(dataset):\n",
    "        filtered_training_data_1 = dataset['1'] == 1.0  #filter data 1\n",
    "        filtered_training_data_5 = dataset['1'] == 5.0 #filter data 5\n",
    "\n",
    "        training_data1_df_1 = dataset[filtered_training_data_1]\n",
    "        training_data1_df_5 = dataset[filtered_training_data_5]\n",
    "        \n",
    "        \n",
    "        frames = [training_data1_df_1,training_data1_df_5]\n",
    "        filtered_df = pd.concat(frames)   #add two frames together\n",
    "        return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered = prepare(training_data1_df)\n",
    "filtered.index = np.arange(1, len(filtered) + 1) # reconstruct filtered data with correct index\n",
    "\n",
    "filtered2 = prepare(training_data2_df)\n",
    "filtered2.index = np.arange(1, len(filtered2) + 1)\n",
    "\n",
    "data_set2 = filtered2.drop(columns=['1'])\n",
    "numpy_data = data_set2.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error(data,label,length,function):\n",
    "    error_count_for_test = 0\n",
    "    for i in range(length):\n",
    "        if(function(data[i,0]) > data[i,1] and label[i] == 1):\n",
    "            error_count_for_test += 1\n",
    "        elif(function(data[i,0]) < data[i,1] and label[i] == -1):\n",
    "            error_count_for_test += 1\n",
    "        else:\n",
    "            error_count_for_test = error_count_for_test\n",
    "\n",
    "    return error_count_for_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(data):\n",
    "        data_set2 = data.drop(columns=['1'])\n",
    "        numpy_data = data_set2.to_numpy()\n",
    "        chcker = data.to_numpy()\n",
    "        length = len(numpy_data)\n",
    "        x1 = np.zeros(length)\n",
    "        negetive = np.zeros(length)\n",
    "        positive = np.zeros(length)# for indensity  in the dataframe -1 is white and 1 is black\n",
    "        x2 = np.zeros(length) # for symmetry\n",
    "        x3 = np.zeros(length)\n",
    "        const = 16\n",
    "\n",
    "        for i in range(length):\n",
    "            for j in range(256):\n",
    "                if(numpy_data[i][j] <= 0):\n",
    "                    negetive[i]+= numpy_data[i][j] #intensity\n",
    "                else:\n",
    "                    positive[i]+= numpy_data[i][j]\n",
    "            x1[i] = 1-abs(negetive[i]/(abs(negetive[i])+positive[i]))\n",
    "#                 x1[i] =1- abs(negetive[i]/256)\n",
    "            \n",
    "        for i in range(length):\n",
    "            image_data = np.reshape(numpy_data[i],(16,16))\n",
    "            for j in range(15):\n",
    "                for x in range(7):\n",
    "                    x2[i] += abs(image_data[j][x]-image_data[j][15-x])\n",
    "                    \n",
    "        for i in range(length):\n",
    "            image_data = np.reshape(numpy_data[i],(16,16))\n",
    "            for j in range(7):\n",
    "                for x in range(15):\n",
    "                    x3[i] += abs(image_data[j][x]-image_data[15-j][x])\n",
    "                        \n",
    "        for i in range(length):\n",
    "            x2[i] = 1-((x2[i]+x3[i])/200)\n",
    "        Data_vector = np.array(list(zip(x1,x2)))\n",
    "        return Data_vector , chcker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1561\n"
     ]
    }
   ],
   "source": [
    "data_training, list_training = get_dataset(filtered)\n",
    "data_test, list_test = get_dataset(filtered2)\n",
    "Y = []\n",
    "\n",
    "length_training = len(data_training)\n",
    "print(length_training)\n",
    "length_test = len(data_test) # get test data set\n",
    "\n",
    "data_training_y = [] #array to label 1 or 5 1 is 1 and -1 is 5\n",
    "Data_set_X = []\n",
    "for i in range(length_training):\n",
    "    if(list_training[i][0] == 1.0):\n",
    "        data_training_y.append(1);\n",
    "    else:\n",
    "        data_training_y.append(-1);\n",
    "for j in range(length_training):\n",
    "    temp = np.matrix(data_training[j])\n",
    "    temp = temp.T\n",
    "    temp = np.array(temp)\n",
    "    temp = np.vstack([[1], temp])\n",
    "    Data_set_X.append(temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[779.83007849]]), array([[1112.]]), array([[1112.]]), array([[1112.]]), array([[1112.]]), array([[1112.]]), array([[1112.]]), array([[1112.]]), array([[1112.]]), array([[1112.]])]\n"
     ]
    }
   ],
   "source": [
    "M=10\n",
    "W1 = initialWeigths(M,3)\n",
    "W2 = initialWeigths(1,M+1)\n",
    "Ein_array = []\n",
    "iteration = []\n",
    "W_array = []\n",
    "W_array.append(W1)\n",
    "W_array.append(W2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(Ein)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
